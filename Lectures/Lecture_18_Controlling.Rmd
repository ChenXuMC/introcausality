---
title: "Lecture 18 Closing Back Doors: Controlling"
author: "Nick Huntington-Klein"
date: "March 6, 2019"
output:   
  revealjs::revealjs_presentation:
    theme: solarized
    transition: slide
    self_contained: true
    smart: true
    fig_caption: true
    reveal_options:
      slideNumber: true
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
library(tidyverse)
library(dagitty)
library(ggdag)
library(gganimate)
library(ggthemes)
library(Cairo)
library(wooldridge)
library(stargazer)
theme_set(theme_gray(base_size = 15))
```

## Recap

- We discussed how to draw a causal diagram
- How to identify the front and back door paths
- And how we can close those back door paths by controlling/adjusting in order to identify the front-door paths we want!
- And so we get our causal effect

## Today

- Today we're going to be going a little deeper into what it means to actually control/adjust for things
- And we're also going to talk about times when controlling/adjusting makes things *WORSE* - collider bias!
- I'm going to just start saying "controlling", by the way - "adjusting" is a little more accurate, but "controlling" is more common

## Controlling

- Up to now, here's how we've been getting the relationship between `X` and `Y` while controlling for `W`:
1. See what part of `X` is explained by `W`, and subtract it out. Call the result the residual part of `X`.
2. See what part of `Y` is explained by `W`, and subtract it out. Call the result the residual part of `Y`.
3. Get the relationship between the residual part of `X` and the residual part of `Y`.
- With the last step including things like getting the correlation, plotting the relationship, calculating the variance explained, or comparing mean `Y` across values of `X`

## In code

```{r, echo=TRUE}
df <- tibble(w = rnorm(100)) %>%
  mutate(x = 2*w + rnorm(100)) %>%
  mutate(y = 1*x + 4*w + rnorm(100))
cor(df$x,df$y)
df <- df %>% group_by(cut(w,breaks=5)) %>%
  mutate(x.resid = x - mean(x),
         y.resid = y - mean(y))
cor(df$x.resid,df$y.resid)
```

## In Diagrams

- The relationship between `X` and `Y` reflects both `X->Y` and `X<-W->Y`
- We remove the part of `X` and `Y` that `W` explains to get rid of `X<-W` and `W->Y`, blocking `X<-W->Y` and leaving `X->Y`
```{r, dev='CairoPNG', echo=FALSE, fig.width=5, fig.height=4}
dag <- dagify(X~W,
              Y~X+W) %>% tidy_dagitty()
ggdag(dag,node_size=20)
```

## More than One Variable

- It's quite possible to control for more than one variable at a time
- Although we won't be doing it much in this class
- A common way to do this is called multiple regression
- You can do it with our method too, but it gets tedious pretty quickly

## More than One Variable

```{r, echo=TRUE}
df <- tibble(w = rnorm(100),v=rnorm(100)) %>%
  mutate(x = 2*w + 3*v + rnorm(100)) %>%
  mutate(y = 1*x + 4*w + 1.5*v + rnorm(100))
cor(df$x,df$y)
df <- df %>% group_by(cut(w,breaks=5)) %>%
  mutate(x.resid = x - mean(x),
         y.resid = y - mean(y)) %>%
  group_by(cut(v,breaks=5)) %>%
  mutate(x.resid2 = x.resid - mean(x.resid),
         y.resid2 = y.resid - mean(y.resid))
cor(df$x.resid2,df$y.resid2)
```

## Graphically

```{r, dev='CairoPNG', echo=FALSE, fig.width=5, fig.height=4.5}
df <- data.frame(W = as.integer((1:200>100))) %>%
  mutate(X = .5+2*W + rnorm(200)) %>%
  mutate(Y = -.5*X + 4*W + 1 + rnorm(200),time="1") %>%
  group_by(W) %>%
  mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%
  ungroup()

#Calculate correlations
before_cor <- paste("1. Start with raw data. Correlation between X and Y: ",round(cor(df$X,df$Y),3),sep='')
after_cor <- paste("6. Correlation between X and Y controlling for W: ",round(cor(df$X-df$mean_X,df$Y-df$mean_Y),3),sep='')


#Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label
dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,time=before_cor),
  #Step 2: Add x-lines
  df %>% mutate(mean_Y=NA,time='2. Figure out what differences in X are explained by W'),
  #Step 3: X de-meaned 
  df %>% mutate(X = X - mean_X,mean_X=0,mean_Y=NA,time="3. Remove differences in X explained by W"),
  #Step 4: Remove X lines, add Y
  df %>% mutate(X = X - mean_X,mean_X=NA,time="4. Figure out what differences in Y are explained by W"),
  #Step 5: Y de-meaned
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=0,time="5. Remove differences in Y explained by W"),
  #Step 6: Raw demeaned data only
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=NA,time=after_cor))

p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(W)))+geom_point()+
  geom_vline(aes(xintercept=mean_X,color=as.factor(W)))+
  geom_hline(aes(yintercept=mean_Y,color=as.factor(W)))+
  guides(color=guide_legend(title="W"))+
  scale_color_colorblind()+
  labs(title = 'The Relationship between Y and X, Controlling for  W \n{next_state}')+
  transition_states(time,transition_length=c(12,32,12,32,12,12),state_length=c(160,100,75,100,75,160),wrap=FALSE)+
  ease_aes('sine-in-out')+
  exit_fade()+enter_fade()

animate(p,nframes=200)
```

## Intuitively

- So what does this actually *mean*? Why do we do it this way?
- As mentioned before, the goal here is to remove `X<-W` and `W->Y` so as to close the back door
- But the way we actually *do* this is by removing differences that are *predicted* by `W`
- In other words, we are are *comparing people as though they had the same value of `W`*

## Intuitively

- That's why you hear some people refer to controlling as "holding `W` constant" - we literally remove the variation in `W`, leaving it "constant"
- Another way of thinking of it is that you're looking for variation of `X` and `Y` *within* values of `W` - this is made clear in the animation
- **Comparing apples to apples**

## Intuitively

- Thinking about it this way also makes it clear that there are other ways to control for things besides the method we've outlined
- Anything that ensures that we're looking at observations with the *same* (or at least very very similar) values of `W` is in effect controlling for `W`
- A common way this happens is by selecting a sample

## An Example

- We'll borrow an example from the Wooldridge econometrics textbook (data available in the `wooldridge` package)
- LaLonde (1986) is a study of whether a job training program improves earnings in 1978 (`re78`)
- Specifically, it has data on an *experiment* of *assigning* people to a job training program (data `jtrain2`)
- And also data on people who *chose* to participate in that program, or didn't (data `jtrain3`)
- The goal of causal inference - do something to `jtrain3` so it gives us the "correct" result from `jtrain2`

## LaLonde

```{r, echo=TRUE, eval=FALSE}
library(wooldridge)
#EXPERIMENT
data(jtrain2)
jtrain2 %>% group_by(train) %>% summarize(wage = mean(re78))
```
```{r, echo=FALSE, eval=TRUE}
#EXPERIMENT
data(jtrain2)
jtrain2 %>% group_by(train) %>% summarize(wage = mean(re78))
```
```{r, echo=TRUE, eval=TRUE}
#BY CHOICE
data(jtrain3)
jtrain3 %>% group_by(train) %>% summarize(wage = mean(re78))
```

## Hmm...

- What back doors might the `jtrain3` analysis be facing?
- People who need training want to get it but are likely to get lower wages anyway!

```{r, dev='CairoPNG', echo=FALSE, fig.width=7, fig.height=4.5}
set.seed(1000)
dag <- dagify(train~need.tr+U,
              wage~train+need.tr+U) %>% tidy_dagitty()
ggdag(dag,node_size=20)
```

## Apples to Apples

- The two data sets are looking at very different groups of people!

```{r, echo=TRUE, eval=FALSE}
library(stargazer)
stargazer(select(jtrain2,re75,re78),type='text')
stargazer(select(jtrain3,re75,re78),type='text')
```
```{r, echo=FALSE, eval=TRUE}
stargazer(select(jtrain2,re75,re78),type='text')
stargazer(select(jtrain3,re75,re78),type='text')
```

## Controlling

- We can't measure "needs training" directly, but we can sort of control for it by limiting ourselves solely to the kind of people who need it - those who had low wages in 1975!

```{r, echo=FALSE, eval=TRUE}
jtrain2 %>% group_by(train) %>% summarize(wage = mean(re78))
jtrain3 %>% filter(re75 <= 1.2) %>% group_by(train) %>% summarize(wage = mean(re78))
```

## Controlling

- Not exactly the same (not surprising - we were pretty arbitrary in how we controlled for `need.tr`, and we never closed `train <- U -> wage`, oh and we left out plenty of other back doors: race, age, etc.) but an improvement
- This goes to show that choosing a sample is a *form* of controlling
- ANYTHING that ensures you're looking at observations with similar values of `W` is a form of controlling for `W`

## Bad Controls

- So far so good - we have the concept of what it means to control and some ways we can do it, so we can get apples-to-apples comparisons
- But what should we control for?
- Everything, right? We want to make sure our comparison is as apple-y as possible!
- Well, no, not actually

## Bad Controls

- Some controls can take you away from showing you the front door
- We already discussed how it's not a good idea to block a front-door path.
- An increase in the price of cigarettes might improve your health, but not if we control for the number of cigarettes you smoke!

```{r, dev='CairoPNG', echo=FALSE, fig.width=5, fig.height=2.5}
dag <- dagify(cigs~price,
              health~cigs,
              coords=list(
                x=c(price=1,cigs=2,health=3),
                y=c(price=1,cigs=1,health=1)
              )) %>% tidy_dagitty()
ggdag(dag,node_size=20)
```

## Bad Controls

- There is another kind of bad control - a *collider*
- Basically, if you're listing out paths, and you see a path where the arrows *collide* by both pointing at the same variable, **that path is already blocked**
- Like this: `X <- W -> C <- Z -> Y`
- Note the `-> C <-`. Those arrow are colliding!
- If we control for the collider `C`, *that path opens back up!*

## Colliders

- One kind of diagram (of many) where this might pop up:

```{r, dev='CairoPNG', echo=FALSE, fig.width=5, fig.height=5}
m_bias(x_y_associated=TRUE) %>%
  ggdag(node_size=20)
```

## Colliders

- How could this be?
- Because even if two variables *cause* the same thing (`a -> m`, `b -> m`), that doesn't make them related. Your parents both caused your genetic makeup, that doesn't make *their* genetics related. Knowing dad's eye color tells you nothing about mom's.
- But *within given values of the collider*, they ARE related. If you're brown-eyed, then observing that your dad has blue eyes tells us that your mom is brown-eyed

## Colliders

- So here, `x <- a -> m <- b -> y` is pre-blocked, no problem. `a` and `b` are unrelated, so no back door issue!
- Control for `m` and now `a` and `b` are related, back door path open.

```{r, dev='CairoPNG', echo=FALSE, fig.width=5, fig.height=4}
m_bias(x_y_associated=TRUE) %>%
  ggdag(node_size=20)
```

## Example

- You want to know if programming skills reduce your social skills
- So you go to a tech company and test all their employees on programming and social skills
- Let's imagine that the *truth* is that programming skills and social skills are unrelated
- But you find a negative relationship! What gives?

## Example

- Oops! By surveying only the tech company, you controlled for "works in a tech company"
- To do that, you need programming skills, social skills, or both! It's a collider!

```{r, dev='CairoPNG', echo=FALSE, fig.width=5, fig.height=3.5}
dag <- dagify(hired~prog,
              hired~social,
              coords=list(
                x=c(prog=1,social=3,hired=2),
                y=c(prog=2,social=2,hired=1)
              )) %>% tidy_dagitty()
ggdag(dag,node_size=20)
```

## Example

```{r, echo=TRUE}
set.seed(14233)
survey <- tibble(prog=rnorm(1000),social=rnorm(1000)) %>%
  mutate(hired = (prog + social > .25))
#Truth
cor(survey$prog,survey$social)

#Controlling by just surveying those hired
cor(filter(survey,hired==1)$prog,filter(survey,hired==1)$social)

#Surveying everyone and controlling with our normal method
survey <- survey %>% group_by(hired) %>%  mutate(p.resid = prog - mean(prog),
         s.resid = social - mean(social)) %>% ungroup()
cor(survey$p.resid,survey$s.resid)
```

## Graphically

```{r, dev='CairoPNG', echo=FALSE, fig.width=5, fig.height=3.5}
#Probably try a few times until the raw correlation looks nice and low
df <- survey %>% 
  transmute(time="1",
         X=prog,Y=social,C=hired) %>%
  group_by(C) %>%
  mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%
  ungroup()

#Calculate correlations
before_cor <- paste("1. Start raw. Correlation between prog and social: ",round(cor(df$X,df$Y),3),sep='')
after_cor <- paste("7. Cor between prog and social controlling for hired: ",round(cor(df$X-df$mean_X,df$Y-df$mean_Y),3),sep='')




#Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label
dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,C=0,time=before_cor),
  #Step 2: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,time='2. Separate data by the values of hired.'),
  #Step 3: Add x-lines
  df %>% mutate(mean_Y=NA,time='3. Figure out what differences in prog are explained by hired'),
  #Step 4: X de-meaned 
  df %>% mutate(X = X - mean_X,mean_X=0,mean_Y=NA,time="4. Remove differences in prog explained by hired"),
  #Step 5: Remove X lines, add Y
  df %>% mutate(X = X - mean_X,mean_X=NA,time="5. Figure out what differences in social are explained by hired"),
  #Step 6: Y de-meaned
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=0,time="6. Remove differences in social explained by hired"),
  #Step 7: Raw demeaned data only
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=NA,time=after_cor))

p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(C)))+geom_point()+
  geom_vline(aes(xintercept=mean_X,color=as.factor(C)))+
  geom_hline(aes(yintercept=mean_Y,color=as.factor(C)))+
  guides(color=guide_legend(title="Hired"))+
  scale_color_colorblind()+
  labs(title = 'Inventing a Correlation by Controlling for hired \n{next_state}',
       x='Programming Skill',
       y='Social Skill')+
  transition_states(time,transition_length=c(1,12,32,12,32,12,12),state_length=c(160,125,100,75,100,75,160),wrap=FALSE)+
  ease_aes('sine-in-out')+
  exit_fade()+enter_fade()

animate(p,nframes=200)
```

## Colliders

- This doesn't just create correlations from nothing, it can also distort causal effects that ARE there
- For example, did you know that height is UNrelated to basketball skill... among NBA players?

```{r, dev='CairoPNG', echo=FALSE, fig.width=5, fig.height=4}
basketball <- read.csv(text='PointsPerGame,HeightInches
                          20.8,75
                          17.6,81
                          12.7,78
                          10.9,76
                          10.7,83
                          10.1,75
                          9,81
                          8.8,82
                          8.8,84
                          8.7,81
                          5.5,75
                          5.5,73
                          3.9,81
                          2.3,84
                          2.1,81
                          1.8,77
                          1,74
                          0.5,80')
ggplot(basketball,aes(x=HeightInches,y=PointsPerGame))+geom_point()+
  labs(x="Height in Inches",
       y="Points Per Game",
       title="Chicago Bulls 2009-10")
#Data from Scott Andrews at StatCrunch
```


## Colliders

- Sometimes, things can get real tricky
- In some cases, the same variable NEEDS to be controlled for to close a back door path, but it's a collider on ANOTHER back door path!
- In those cases you just can't identify the effect, at least not easily
- This pops up in estimates of the gender wage gap - example from Cunningham's Mixtape: should you control for occupation when looking at gender discrimination in the labor market?

## Colliders in the Gender Wage Gap

- We are interested in `gender -> discrim -> wage`; our treatment is `gender -> discrim`, the discrimination caused by your gender

```{r, dev='CairoPNG', echo=FALSE, fig.width=5, fig.height=4.5}
dag <- dagify(occup~gender+abil+discrim,
              wage~abil+discrim+occup,
              discrim~gender,
              coords=list(
                x=c(gender=1,discrim=2,occup=2,wage=3,abil=3),
                y=c(gender=2,occup=1,discrim=3,wage=2,abil=1)
              )) %>% tidy_dagitty()
ggdag(dag,node_size=20)
```

## Colliders in the Gender Wage Gap

- <span style = "color:red">Front doors</span>/<span style = "color:blue">Open back doors</span>/<span style = "color:orange">Closed back doors</span>
- <span style = "color:red">`gender -> discrim -> wage`</span>
- <span style = "color:red">`gender -> discrim -> occup -> wage`</span>
- <span style = "color:blue">`discrim <- gender -> occup -> wage`</span>
- <span style = "color:orange">`discrim <- gender -> occup <- abil -> wage`</span>
- <span style = "color:orange">`gender -> discrim -> occup <- abil -> wage`</span>

## Colliders in the Gender Wage Gap

- No `occup` control? Ignore nondiscriminatory reasons to choose different occupations by gender
- Control for `occup`? Open both back doors, create a correlation between `abil` and `discrim` where there wasn't one
- And also close a FRONT door, `gender -> discrim -> occup -> wage`: discriminatory reasons for gender diffs in `occup`
- We actually *can't* identify the effect we want in this diagram by controlling. It happens!
- Suggests this question goes beyond just controlling for stuff. Real research on this topic gets clever.

## Next Time

- Get ready! Next time we'll begin our trek down the list of common causal inference methods as they actually get used!
- Many of them apply controlling for stuff in interesting ways
- Others use methods other than controlling!
- This is what economists and many data scientists actually do with their time
- We will begin with "fixed effects"